{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Multi-label classification model for furniture type and style\n",
    "* We will use transfer learning to train a classification model to obtain a feature layer for similarity identification.\n",
    "\n",
    "### Dataset\n",
    "* Hozzu dataset with 90,298 images from 6 categories of furniture across 17 different styles. \n",
    "* Source: https://cvml.comp.nus.edu.sg/furniture/index.html\n",
    "* We have limited the data to 200 images on 15 styles\n",
    "* in total 21 classes for prediction\n",
    "\n",
    "the model will predict the style of the furniture\n",
    "then we save the model and use its one before last layer as embeding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from imutils import paths\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "          \n",
    "#openCV\n",
    "import cv2                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor Flow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import vgg16\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.applications import xception\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D,Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['tables', 'sofas', 'lamps', 'chairs', 'dressers', 'beds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the tag and images from csv\n",
    "pathroot = \"data/\"\n",
    "tags= []\n",
    "images = []\n",
    "path = []\n",
    "\n",
    "for ptype in labels:\n",
    "    try:\n",
    "        for files in os.listdir(pathroot+ptype):\n",
    "            paths = pathroot+ptype+\"/\"+files\n",
    "            if os.path.isdir(paths):\n",
    "                for filename in os.listdir(paths):\n",
    "                    if filename.endswith(\".jpg\"):\n",
    "                        img_path = os.path.join(paths, filename)\n",
    "                        img_arr = cv2.imread(img_path)\n",
    "                        img_arr = cv2.resize(img_arr,(100,100))\n",
    "                        path.append(ptype+\"/\"+files+\"/\"+filename)\n",
    "                        images.append(img_arr)\n",
    "                        tags.append([ptype,files.lower()])\n",
    "                    else:\n",
    "                        continue\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images,tags = shuffle(images,tags,random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Transfer learning\n",
    "* Train on a smaller dataset \n",
    "* Taking features learned from ImageNet dataset\n",
    "* Dataset of over 14 million images belonging to 1000 classes\n",
    "\n",
    "\n",
    "### Multi-label classification\n",
    "* Multi-label classification is a predictive modeling involves predicting zero or more mutually non-exclusive class labels \n",
    "* Multi Label Binarizer: transform the classes to list of binary number using multi label binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "tags = mlb.fit_transform(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(mlb.classes_)).to_csv('classes.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocation of train/test/validation data\n",
    "* 70% Training 15% Testing 15% Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullsize = len(images)\n",
    "trainsize = int(fullsize*0.7)\n",
    "testsize = int(trainsize+fullsize*0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(images[0:trainsize])\n",
    "y_train=np.array(tags[0:trainsize])\n",
    "X_test=np.array(images[trainsize:testsize])\n",
    "y_test=np.array(tags[trainsize:testsize])\n",
    "X_val=np.array(images[testsize:])\n",
    "y_val=np.array(tags[testsize:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epo = 30\n",
    "init_lr = 1e-3\n",
    "bs = 32\n",
    "image_dims = (100, 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Image Data Generator for Image augmentation\n",
    "*  Flow method: Takes data & label arrays, generates batches of augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=bs)\n",
    "test_generator = test_datagen.flow(X_test, y_test, batch_size=bs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Overview\n",
    "#### VGG16 with weights imagenet\n",
    "* Model 1: VGG16 with all layers freezed, one 256 nodes fully-connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 -  VGG16 with all layers freezed, one 256 nodes fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "base_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(100, 100, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = Dropout(0.5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256,activation='relu')(x)\n",
    "predictions = Dense(21, activation='sigmoid')(x)\n",
    "\n",
    "model0 = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "print (model0.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=init_lr)\n",
    "model0.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])    \n",
    "\n",
    "cp_callback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "               tf.keras.callbacks.ModelCheckpoint(filepath='model0.{epoch:02d}-{val_loss:.2f}.h5',verbose=1),\n",
    "               tf.keras.callbacks.TensorBoard(log_dir='./logs'),]\n",
    "\n",
    "# Train the model with the new callback\n",
    "\n",
    "hist0 = model0.fit(train_generator,steps_per_epoch=len(X_train)/bs,\n",
    "                 epochs=epo,\n",
    "                 validation_data=test_generator,\n",
    "                 callbacks=cp_callback,verbose=1)  # Pass callback to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.save(\"multilabel0\")\n",
    "\n",
    "with open('trainhist0', 'wb') as file_pi:\n",
    "      pickle.dump(hist0.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Plot data to see relationships in training and validation data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot') \n",
    "%matplotlib inline\n",
    "\n",
    "epoch_list = list(range(1, len(hist0.history['accuracy']) + 1))  # values for x axis [1, 2, ..., # of epochs]\n",
    "plt.plot(epoch_list, hist0.history['accuracy'], epoch_list, hist0.history['val_accuracy'])\n",
    "plt.legend(('Training Accuracy', 'Validation Accuracy'))\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "epoch_list = list(range(1, len(hist0.history['loss']) + 1))  # values for x axis [1, 2, ..., # of epochs]\n",
    "plt.plot(epoch_list, hist0.history['loss'], epoch_list, hist0.history['val_loss'])\n",
    "plt.legend(('Training Loss', 'Validation Loss'))\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list = []\n",
    "val_generator = test_datagen.flow(X_val, y_val, batch_size=bs)\n",
    "\n",
    "#   Evaluate the model with the test data to get the scores on \"real\" data.\n",
    "score0 = model0.evaluate(val_generator, verbose=0)\n",
    "\n",
    "print('Test loss:', score0[0])\n",
    "print('Test accuracy:', score0[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from now to the rest there was more models to try but i deleted them because they are not necessery"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
